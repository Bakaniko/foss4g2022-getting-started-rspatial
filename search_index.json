[["index.html", "Getting started with R and R-spatial About", " Getting started with R and R-spatial Nicolas Roelandt and Jakub Nowosad 2022-07-30 About This is the teaching materials for a workshop introducing R and R-spatial at the FOSS4G2022 conference. "],["introduction.html", "Introduction Schedule Pre-requisites What is love R ? 0.1 Why an R workshop in a FOSS4G conference ? Coding paradigms", " Introduction This workshops aims to provide the very basics of R to newcomers and a first introduction to spatial data handling in R using the R-spatial ecosystem. We will present to the audience mapping with the {tmap} package, vector data handling with {sf} and raster data handling with {terra}. No previous knowledge of R is required. The workshop leaders are Nicolas Roelandt from Gustave Eiffel university (Lyon, France) and Jakub Nowosad from Adam Mickiewicz University (Poznań, Poland). Schedule Opening the workshop (5 min) Introduction to R (15 min) Non-spatial data handling (30 min) Introduction to the R-spatial ecosystem (20 min) Mapping in R with {tmap} (30 min) Coffee break (to be confirmed) Vector data processing (45 min) Raster data processing (45 min) Closing the workshop (5 min) Pre-requisites A working installation of R (following CRAN recommandations for your computer) R comes with the RGui interface. It is usable but for a better user experience we recommend using Rstudio. Jupyter Notebook with the IRKernel can also be a good option. R packages Please run those commands into the R console: packages &lt;- c( gapminder, tmaptools, tmap, spData, sf, terra, fields, supercells, here) # Install packages from CRAN Repo install.packages(packages) # Install spDataLarge that is not on CRAN repo install.packages(&quot;spDataLarge&quot;, repos = &quot;https://geocompr.r-universe.dev&quot;) What is love R ? R is a programming language dedicated to data science. It can compute statistics and produce graphics and reports (and much more). It was created by Ross Ihaka and Robert Gentleman in 1993 and was released as a Free and Open Source Software in 1995. 0.1 Why an R workshop in a FOSS4G conference ? While R is not dedicated to spatial analysis, there a several dozen of packages that provides geospatial capabilities to the language. Use R a standalone GIS: read and write geospatial data connect to geospatial data base perform spatial analysis make geostatistical modelling, prediction and simulation access to algorithms from other tools (QGIS, GRASS, SAGA) Use R in other FOSS4G tools: QGIS GRASS Coding paradigms When R was released, there was no strong syntax philosophy so there are some inconsistencies in packages, functions and arguments naming, for example. R base readability and performance where not good enough for some users so they developed packages to improve those. When using R for data analysis, you will encounter 3 majors coding paradigms: base R Tidyverse data.table Base R is a vanilla R code. The tidyverse aims to provide a more consistent grammar and readability. Data.table provides a fast and powerful alternative to R base with a consistent grammar. You can mix those paradigms for your projects, but for teaching purposes the workshop materials will use tidyverse with some base R. "],["data-handling-with-r.html", "Chapter 1 Data handling with R 1.1 Data types 1.2 Assignment 1.3 Not only a calculator 1.4 Packages 1.5 Load data 1.6 In the beginning was the Verb 1.7 Filter data 1.8 Select columns 1.9 Create new variables 1.10 Agregate data 1.11 Join data 1.12 Piping", " Chapter 1 Data handling with R 1.1 Data types numeric: 2 12.125 character strings: 'a', \"word\" logical: TRUE FALSE NULL (sort of) Please note that logicals are in capital letters. vectors: c(12, 15, 35698) c(\"FOSS4G 2022\", \"Firenze\") list: list(1, 45, 12.0, \"toto\") matrices: matrix(0:9, 3,3) data frames (df): data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")) constants: letters[3:4] LETTERS[12:26] pi 1.2 Assignment Use of variables to store data in memory. Use &lt;- (or =) Examples: a &lt;- c(0, 1, 2) # new integer vector b &lt;- c(&quot;FOSS4G&quot;, &quot;2022&quot;, &quot;Firenze&quot;) # new character vector c &lt;- data.frame( number = a, strings = b) d &lt;- list(1, 45, 12.0, &quot;toto&quot;) # lists can store different data types e = matrix(0:9, 3,3) 1.3 Not only a calculator R is shipped with lots of functions like : data(&lt;datasetname&gt;) # load an embedded dataset head(&lt;objectname&gt;) # first lines of a dataframe is.vector(object) # return TRUE if object is a vector is.data.frame(object) # return TRUE if object is a data.frame class(&lt;objectname&gt;) # Returns the class of an object unique(&lt;objectname&gt;) # returns unique values help(&lt;functionname&gt;) # get help on a function plot(&lt;objectname&gt;) # create a graphic from a dataset Exercises : Load the LakeHuron dataset using data() Get help on the LakeHuron dataset Use head() to see its first lines Plot LakeHuron 1.4 Packages While base R contains a lot of functions, it can be extended with packages. In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. Wickham and Bryan (2022) You already saw how to install packages with the install.packages() function, let’s see how to load the {dplyr} package we will use to handle the data. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 1.5 Load data If the packages do not provide datasets to work with, you will want to work on your own data. There is several ways to load data. Base R provides a set of functions for delimited text files. For example, if we want to work with the Gapminder dataset from Software Carpentry’s R course. data_url &lt;- &quot;https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/main/data/gapminder_data.csv&quot; # download the data and store it in a variable gapminder &lt;- read.csv(data_url) We call this dataset from an URL but it can be a path in your system file. For example, if you have installed the gapminder package as recommended, you will find the data set as a TSV file in your R installation. data_url &lt;- system.file(&quot;extdata/gapminder.tsv&quot;, package = &quot;gapminder&quot;) data_url ## [1] &quot;/Users/runner/work/_temp/Library/gapminder/extdata/gapminder.tsv&quot; system.file() is a function that the path of files in R packages, independently of the operating system. gapminder &lt;- read.delim(data_url, sep = &quot;\\t&quot;) read.csv() and similar functions can read delimited text files only. For other formats, you can use functions from other packages like Haven or readxl. We will show in Chapter 2 how to load geospatial data. Let’s take a look to our data : glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ pop &lt;dbl&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… ## $ continent &lt;chr&gt; &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asia&quot;, &quot;Asi… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … This dataset is about life expectancy, population and GDP per capita in world countries between 1952 and 2007. You can also use head() for the same purpose but a different output format. head(gapminder) ## country year pop continent lifeExp gdpPercap ## 1 Afghanistan 1952 8425333 Asia 28.801 779.4453 ## 2 Afghanistan 1957 9240934 Asia 30.332 820.8530 ## 3 Afghanistan 1962 10267083 Asia 31.997 853.1007 ## 4 Afghanistan 1967 11537966 Asia 34.020 836.1971 ## 5 Afghanistan 1972 13079460 Asia 36.088 739.9811 ## 6 Afghanistan 1977 14880372 Asia 38.438 786.1134 We can transform a dataframe into a tibble to access their better print() method that combines head() and glimpse(). Tibbles are at the core of the Tidyverse packages. gapminder &lt;- as_tibble(gapminder) gapminder ## # A tibble: 1,704 × 6 ## country year pop continent lifeExp gdpPercap ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 8425333 Asia 28.8 779. ## 2 Afghanistan 1957 9240934 Asia 30.3 821. ## 3 Afghanistan 1962 10267083 Asia 32.0 853. ## 4 Afghanistan 1967 11537966 Asia 34.0 836. ## 5 Afghanistan 1972 13079460 Asia 36.1 740. ## 6 Afghanistan 1977 14880372 Asia 38.4 786. ## 7 Afghanistan 1982 12881816 Asia 39.9 978. ## 8 Afghanistan 1987 13867957 Asia 40.8 852. ## 9 Afghanistan 1992 16317921 Asia 41.7 649. ## 10 Afghanistan 1997 22227415 Asia 41.8 635. ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows If you want more information about the gapminder dataset, Hans Rosling made a TED talk presenting the gapminder data. We can see that it contains several columns (also called variables): names(gapminder) ## [1] &quot;country&quot; &quot;year&quot; &quot;pop&quot; &quot;continent&quot; &quot;lifeExp&quot; &quot;gdpPercap&quot; 1.6 In the beginning was the Verb {dplyr} provides a lot of functions to handle data: filter() to filter the data matching certain conditions (subset data), select() to select columns, mutate() to create new variables. All those functions are verbs and means an action onto the dataset. 1.7 Filter data If we only want the records from Italy, we can filter using: filter(gapminder, country == &quot;Italy&quot; ) ## # A tibble: 12 × 6 ## country year pop continent lifeExp gdpPercap ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Italy 1952 47666000 Europe 65.9 4931. ## 2 Italy 1957 49182000 Europe 67.8 6249. ## 3 Italy 1962 50843200 Europe 69.2 8244. ## 4 Italy 1967 52667100 Europe 71.1 10022. ## 5 Italy 1972 54365564 Europe 72.2 12269. ## 6 Italy 1977 56059245 Europe 73.5 14256. ## 7 Italy 1982 56535636 Europe 75.0 16537. ## 8 Italy 1987 56729703 Europe 76.4 19207. ## 9 Italy 1992 56840847 Europe 77.4 22014. ## 10 Italy 1997 57479469 Europe 78.8 24675. ## 11 Italy 2002 57926999 Europe 80.2 27968. ## 12 Italy 2007 58147733 Europe 80.5 28570. You can use comparison operators like ==, &gt;, &lt;, &gt;= , etc. There is also logical operators : &amp; (AND), | (OR), ! (NOT) and xor(). So, for example, if we want to subset records for Italy after 2000, we can use filter like this: italy_2000 &lt;- filter(gapminder, country == &quot;Italy&quot; &amp; year &gt; 2000 ) italy_2000 ## # A tibble: 2 × 6 ## country year pop continent lifeExp gdpPercap ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Italy 2002 57926999 Europe 80.2 27968. ## 2 Italy 2007 58147733 Europe 80.5 28570. Exercice Try to subset non-European records Try to subset records that are in Oceania or before 2000 1.8 Select columns select() allows you to keep only the columns you need for your analysis. Maybe we only want the country, year and lifeExp variables: select(italy_2000, country, year, lifeExp) ## # A tibble: 2 × 3 ## country year lifeExp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Italy 2002 80.2 ## 2 Italy 2007 80.5 Or, for example, in the italy_2000 subset, the continent variable does not provide useful information anymore, so we want to discard it. Please not the use of the - symbol before the name of the variable. select(italy_2000, -continent) ## # A tibble: 2 × 5 ## country year pop lifeExp gdpPercap ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Italy 2002 57926999 80.2 27968. ## 2 Italy 2007 58147733 80.5 28570. So you can select the column you want to keep or the ones you want to remove. 1.9 Create new variables Let say you want to compute the GDP, in millions, from the population and the GDP per capita. For that, you can use the mutate() function: mutate(gapminder, GPD = (gdpPercap * pop)/ 1000000) ## # A tibble: 1,704 × 7 ## country year pop continent lifeExp gdpPercap GPD ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 8425333 Asia 28.8 779. 6567. ## 2 Afghanistan 1957 9240934 Asia 30.3 821. 7585. ## 3 Afghanistan 1962 10267083 Asia 32.0 853. 8759. ## 4 Afghanistan 1967 11537966 Asia 34.0 836. 9648. ## 5 Afghanistan 1972 13079460 Asia 36.1 740. 9679. ## 6 Afghanistan 1977 14880372 Asia 38.4 786. 11698. ## 7 Afghanistan 1982 12881816 Asia 39.9 978. 12599. ## 8 Afghanistan 1987 13867957 Asia 40.8 852. 11821. ## 9 Afghanistan 1992 16317921 Asia 41.7 649. 10596. ## 10 Afghanistan 1997 22227415 Asia 41.8 635. 14122. ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows It’s companion function, transmute() does the same thing but only keep the new variables. transmute(gapminder, GPD = (gdpPercap * pop)/ 1000000) ## # A tibble: 1,704 × 1 ## GPD ## &lt;dbl&gt; ## 1 6567. ## 2 7585. ## 3 8759. ## 4 9648. ## 5 9679. ## 6 11698. ## 7 12599. ## 8 11821. ## 9 10596. ## 10 14122. ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows If you want to keep variables from the dataset, you can call them in the function call: transmute(gapminder, country, year, GPD = (gdpPercap * pop)/ 1000000) ## # A tibble: 1,704 × 3 ## country year GPD ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 6567. ## 2 Afghanistan 1957 7585. ## 3 Afghanistan 1962 8759. ## 4 Afghanistan 1967 9648. ## 5 Afghanistan 1972 9679. ## 6 Afghanistan 1977 11698. ## 7 Afghanistan 1982 12599. ## 8 Afghanistan 1987 11821. ## 9 Afghanistan 1992 10596. ## 10 Afghanistan 1997 14122. ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows So in this example, we compute the GPD but we also keep the information about the country and the year. 1.10 Agregate data Sometimes, we have a too detailed dataset and we want a more broader view of the data so we want to aggregate it. To do so, dplyr provides a couple of functions: group_by() and summarise(). Like their names says, group_by() groups records who share the same value in a variable and summarise() compute the summary of non-grouping variables. For example, let’s compute the yearly population of each continent. To do this, we first group the data by continent and year and pass the result to summarise(). Into this second function, we create a new variable called population that is the sum of the variable pop of each group. summarise(group_by(gapminder, continent, year), population = sum(pop)) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the ## `.groups` argument. ## # A tibble: 60 × 3 ## # Groups: continent [5] ## continent year population ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Africa 1952 237640501 ## 2 Africa 1957 264837738 ## 3 Africa 1962 296516865 ## 4 Africa 1967 335289489 ## 5 Africa 1972 379879541 ## 6 Africa 1977 433061021 ## 7 Africa 1982 499348587 ## 8 Africa 1987 574834110 ## 9 Africa 1992 659081517 ## 10 Africa 1997 743832984 ## # … with 50 more rows ## # ℹ Use `print(n = ...)` to see more rows You can use any function to summarise your data, for example, if you want to know the number of entries by continent, you can use n(). summarise(group_by(gapminder, continent), count = n()) ## # A tibble: 5 × 2 ## continent count ## &lt;chr&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Or if you only want the first country of each continent: summarise(group_by(gapminder, continent), country = first(.data[[&quot;country&quot;]])) ## # A tibble: 5 × 2 ## continent country ## &lt;chr&gt; &lt;chr&gt; ## 1 Africa Algeria ## 2 Americas Argentina ## 3 Asia Afghanistan ## 4 Europe Albania ## 5 Oceania Australia .data is a pronoun that you can use when the column name is a character vector. 1.11 Join data dplyr provides a large variety of functions to join datasets : inner_join(),left_join(), right_join(), full_join(), nest_join() , semi_join(),anti_join(). Let’s create two datasets that shares a common key. gapminder_left &lt;- transmute(gapminder, key = paste0(country, &quot;_&quot; , year), country, continent, year) gapminder_left ## # A tibble: 1,704 × 4 ## key country continent year ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan_1952 Afghanistan Asia 1952 ## 2 Afghanistan_1957 Afghanistan Asia 1957 ## 3 Afghanistan_1962 Afghanistan Asia 1962 ## 4 Afghanistan_1967 Afghanistan Asia 1967 ## 5 Afghanistan_1972 Afghanistan Asia 1972 ## 6 Afghanistan_1977 Afghanistan Asia 1977 ## 7 Afghanistan_1982 Afghanistan Asia 1982 ## 8 Afghanistan_1987 Afghanistan Asia 1987 ## 9 Afghanistan_1992 Afghanistan Asia 1992 ## 10 Afghanistan_1997 Afghanistan Asia 1997 ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows gapminder_right &lt;- transmute(gapminder, key = paste0(country, &quot;_&quot; , year), lifeExp, pop, gdpPercap) gapminder_right ## # A tibble: 1,704 × 4 ## key lifeExp pop gdpPercap ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan_1952 28.8 8425333 779. ## 2 Afghanistan_1957 30.3 9240934 821. ## 3 Afghanistan_1962 32.0 10267083 853. ## 4 Afghanistan_1967 34.0 11537966 836. ## 5 Afghanistan_1972 36.1 13079460 740. ## 6 Afghanistan_1977 38.4 14880372 786. ## 7 Afghanistan_1982 39.9 12881816 978. ## 8 Afghanistan_1987 40.8 13867957 852. ## 9 Afghanistan_1992 41.7 16317921 649. ## 10 Afghanistan_1997 41.8 22227415 635. ## # … with 1,694 more rows ## # ℹ Use `print(n = ...)` to see more rows Now we can join them. joined_gapminder &lt;- left_join( gapminder_left, gapminder_right, by = &quot;key&quot; # optional argument if the join variables have the same name ) Left Join animation (Copyright Garrick Aden_Buie) If you want more information on joins operations with dplyr, we recommend to read the dedicated blogpost from Garrick Aden-Buie 1.12 Piping Piping allows to create a sequence of actions on a dataset without storing intermediate results. As it can be difficult to debug piped commands for beginners, we won’t use it in this workshop. However, its usage is very frequent so it is most likely that a beginner will encounter it in documentation or in code source publicly available. The most common form %&gt;% is provided by the package {magrittr}, which is part of the tidyverse and is a dependency of {dplyr}, so you don’t have to load it. In the following example, we show how to compute the mean GDP by decade of Europeans countries using pipes to chain functions. gapminder %&gt;% # pass the datset as the first argument filter(continent == &quot;Europe&quot;) %&gt;% # subset on European records select(-continent) %&gt;% # remove the continent column mutate(decade = year - year %% 10, # compute decade GDP = (gdpPercap * pop) / 1000000 # compute GDP ) %&gt;% group_by(country, decade) %&gt;% # grouping variables summarise(mean_GDP = mean(GDP)) # compute mean GDP of the decade ## `summarise()` has grouped output by &#39;country&#39;. You can override using the ## `.groups` argument. ## # A tibble: 180 × 3 ## # Groups: country [30] ## country decade mean_GDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1950 2461. ## 2 Albania 1960 4737. ## 3 Albania 1970 8182. ## 4 Albania 1980 10796. ## 5 Albania 1990 9627. ## 6 Albania 2000 18765. ## 7 Austria 1950 52056. ## 8 Austria 1960 85666. ## 9 Austria 1970 137585. ## 10 Austria 1980 171559. ## # … with 170 more rows ## # ℹ Use `print(n = ...)` to see more rows With R 4.1.0, the built-in piping operator |&gt; has been introduced: gapminder |&gt; # pass the datset as the first argument filter(continent == &quot;Europe&quot;) |&gt; # subset on European records select(-continent) |&gt; # remove the continent column mutate(decade = year - year %% 10, # compute decade GDP = (gdpPercap * pop) / 1000000 # compute GDP ) |&gt; group_by(country, decade) |&gt; # grouping variables summarise(mean_GDP = mean(GDP)) # compute mean GDP of the decade ## `summarise()` has grouped output by &#39;country&#39;. You can override using the ## `.groups` argument. ## # A tibble: 180 × 3 ## # Groups: country [30] ## country decade mean_GDP ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1950 2461. ## 2 Albania 1960 4737. ## 3 Albania 1970 8182. ## 4 Albania 1980 10796. ## 5 Albania 1990 9627. ## 6 Albania 2000 18765. ## 7 Austria 1950 52056. ## 8 Austria 1960 85666. ## 9 Austria 1970 137585. ## 10 Austria 1980 171559. ## # … with 170 more rows ## # ℹ Use `print(n = ...)` to see more rows Those two pipes operators are not equivalent so we recommend to read this R-bloggers’ blogpost on pipe’s operators. References "],["rspatial.html", "Chapter 2 Introduction to the R-spatial ecosystem 2.1 R’s spatial ecosystem(s) 2.2 Vector data 2.3 Raster data", " Chapter 2 Introduction to the R-spatial ecosystem 2.1 R’s spatial ecosystem(s) R is a powerful language for geocomputation, allowing for: Exploratory data analysis (EDA) Data processing (e.g., adding new variables) Data transformation (e.g., changing CRS, reducing size via simplification/aggregation) Data visualization Web application development Software development (e.g., to share new methods) There are many ways to handle geographic data in R, with several dozens of packages in the area. It includes: {sf}, {sp}, {terra}, {raster}, {stars} - spatial classes {dplyr}, {rmapshaper} - processing of attribute tables/geometries {rnaturalearth}, {osmdata} - spatial data download {rgrass}, {qgisprocess}, {link2GI} - connecting with GIS software {gstat}, {mlr3}, {CAST} - spatial data modeling {rasterVis}, {tmap}, {ggplot2} - static visualizations {leaflet}, {mapview}, {mapdeck} - interactive visualizations {spatstat}, {spdep}, {spatialreg}, {dismo}, {landscapemetrics}, {RStoolbox}, {rayshader}, {gdalcubes}, {sfnetworks} - different types of spatial data analysis many more… Visit https://cran.r-project.org/view=Spatial to get an overview of different spatial tasks that can be solved using R. We think it is helpful to start learning the R-spatial ecosystem by using its packages for spatial classes handling. They offer capabilities to read/write spatial data, but also have many tools to process and transform the data. The below figure shows the change in popularity of the main R packages for spatial data handling – in this workshop, we will focus on {sf} for working with spatial vector data and {terra} for working with spatial raster data. Figure 2.1: Source: https://geocompr.robinlovelace.net 2.2 Vector data The {sf} package based on the OGC standard Simple Features. It allows to read, process, visualize, and write various spatial data files. As you can see in Figure 2.2, the {sf} package does not exist in void. Figure 2.2: Source: https://www.r-spatial.org/r/2020/03/17/wkt.html First, it is built upon several external libraries, including GDAL, PROJ, GEOS, and s2geometry. Second, it uses several R packages, such as {s2}, {units}, {DBI}. Third, it is a basis of a few hundred of other R packages. {sf} represent all common vector geometry types (Figure ??): points, lines, polygons and their respective ‘multi’ versions. It also also supports geometry collections. Most of the functions in this package start with a prefix st_: library(sf) ## Linking to GEOS 3.10.2, GDAL 3.4.2, PROJ 7.2.0; sf_use_s2() is TRUE ls(&quot;package:sf&quot;) ## [1] &quot;%&gt;%&quot; &quot;as_Spatial&quot; ## [3] &quot;dbDataType&quot; &quot;dbWriteTable&quot; ## [5] &quot;gdal_addo&quot; &quot;gdal_create&quot; ## [7] &quot;gdal_crs&quot; &quot;gdal_extract&quot; ## [9] &quot;gdal_inv_geotransform&quot; &quot;gdal_metadata&quot; ## [11] &quot;gdal_polygonize&quot; &quot;gdal_rasterize&quot; ## [13] &quot;gdal_read&quot; &quot;gdal_read_mdim&quot; ## [15] &quot;gdal_subdatasets&quot; &quot;gdal_utils&quot; ## [17] &quot;gdal_write&quot; &quot;gdal_write_mdim&quot; ## [19] &quot;get_key_pos&quot; &quot;NA_agr_&quot; ## [21] &quot;NA_bbox_&quot; &quot;NA_crs_&quot; ## [23] &quot;NA_m_range_&quot; &quot;NA_z_range_&quot; ## [25] &quot;pivot_wider.sf&quot; &quot;plot_sf&quot; ## [27] &quot;rawToHex&quot; &quot;read_sf&quot; ## [29] &quot;sf_add_proj_units&quot; &quot;sf_extSoftVersion&quot; ## [31] &quot;sf_proj_info&quot; &quot;sf_proj_network&quot; ## [33] &quot;sf_proj_pipelines&quot; &quot;sf_proj_search_paths&quot; ## [35] &quot;sf_project&quot; &quot;sf_use_s2&quot; ## [37] &quot;sf.colors&quot; &quot;st_agr&quot; ## [39] &quot;st_agr&lt;-&quot; &quot;st_area&quot; ## [41] &quot;st_as_binary&quot; &quot;st_as_grob&quot; ## [43] &quot;st_as_s2&quot; &quot;st_as_sf&quot; ## [45] &quot;st_as_sfc&quot; &quot;st_as_text&quot; ## [47] &quot;st_axis_order&quot; &quot;st_bbox&quot; ## [49] &quot;st_bind_cols&quot; &quot;st_boundary&quot; ## [51] &quot;st_buffer&quot; &quot;st_cast&quot; ## [53] &quot;st_centroid&quot; &quot;st_collection_extract&quot; ## [55] &quot;st_combine&quot; &quot;st_contains&quot; ## [57] &quot;st_contains_properly&quot; &quot;st_convex_hull&quot; ## [59] &quot;st_coordinates&quot; &quot;st_covered_by&quot; ## [61] &quot;st_covers&quot; &quot;st_crop&quot; ## [63] &quot;st_crosses&quot; &quot;st_crs&quot; ## [65] &quot;st_crs&lt;-&quot; &quot;st_delete&quot; ## [67] &quot;st_difference&quot; &quot;st_dimension&quot; ## [69] &quot;st_disjoint&quot; &quot;st_distance&quot; ## [71] &quot;st_drivers&quot; &quot;st_drop_geometry&quot; ## [73] &quot;st_equals&quot; &quot;st_equals_exact&quot; ## [75] &quot;st_filter&quot; &quot;st_geometry&quot; ## [77] &quot;st_geometry_type&quot; &quot;st_geometry&lt;-&quot; ## [79] &quot;st_geometrycollection&quot; &quot;st_graticule&quot; ## [81] &quot;st_inscribed_circle&quot; &quot;st_interpolate_aw&quot; ## [83] &quot;st_intersection&quot; &quot;st_intersects&quot; ## [85] &quot;st_is&quot; &quot;st_is_empty&quot; ## [87] &quot;st_is_longlat&quot; &quot;st_is_simple&quot; ## [89] &quot;st_is_valid&quot; &quot;st_is_within_distance&quot; ## [91] &quot;st_jitter&quot; &quot;st_join&quot; ## [93] &quot;st_layers&quot; &quot;st_length&quot; ## [95] &quot;st_line_merge&quot; &quot;st_line_sample&quot; ## [97] &quot;st_linestring&quot; &quot;st_m_range&quot; ## [99] &quot;st_make_grid&quot; &quot;st_make_valid&quot; ## [101] &quot;st_minimum_rotated_rectangle&quot; &quot;st_multilinestring&quot; ## [103] &quot;st_multipoint&quot; &quot;st_multipolygon&quot; ## [105] &quot;st_nearest_feature&quot; &quot;st_nearest_points&quot; ## [107] &quot;st_node&quot; &quot;st_normalize&quot; ## [109] &quot;st_overlaps&quot; &quot;st_point&quot; ## [111] &quot;st_point_on_surface&quot; &quot;st_polygon&quot; ## [113] &quot;st_polygonize&quot; &quot;st_precision&quot; ## [115] &quot;st_precision&lt;-&quot; &quot;st_read&quot; ## [117] &quot;st_read_db&quot; &quot;st_relate&quot; ## [119] &quot;st_reverse&quot; &quot;st_sample&quot; ## [121] &quot;st_segmentize&quot; &quot;st_set_agr&quot; ## [123] &quot;st_set_crs&quot; &quot;st_set_geometry&quot; ## [125] &quot;st_set_precision&quot; &quot;st_sf&quot; ## [127] &quot;st_sfc&quot; &quot;st_shift_longitude&quot; ## [129] &quot;st_simplify&quot; &quot;st_snap&quot; ## [131] &quot;st_sym_difference&quot; &quot;st_touches&quot; ## [133] &quot;st_transform&quot; &quot;st_triangulate&quot; ## [135] &quot;st_union&quot; &quot;st_viewport&quot; ## [137] &quot;st_voronoi&quot; &quot;st_within&quot; ## [139] &quot;st_wrap_dateline&quot; &quot;st_write&quot; ## [141] &quot;st_write_db&quot; &quot;st_z_range&quot; ## [143] &quot;st_zm&quot; &quot;vec_cast.sfc&quot; ## [145] &quot;vec_ptype2.sfc&quot; &quot;write_sf&quot; Most often, the first step in spatial data analysis in R is to read the data. Here, we will read example file stored in the {spData} package: file_path = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) file_path ## [1] &quot;/Users/runner/work/_temp/Library/spData/shapes/world.gpkg&quot; To read a spatial vector file, we just need to provide a file path to the read_sf() function:1 world = read_sf(file_path) Our new object is an extended data frame with several non-spatial attributes and one special column geom. When we print the object, it also provides a header with some basic spatial information: world ## Simple feature collection with 177 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513 ## Geodetic CRS: WGS 84 ## # A tibble: 177 × 11 ## iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type area_…⁵ pop lifeExp gdpPe…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FJ Fiji Oceania Oceania Melane… Sove… 1.93e4 8.86e5 70.0 8222. ## 2 TZ Tanzania Africa Africa Easter… Sove… 9.33e5 5.22e7 64.2 2402. ## 3 EH Western… Africa Africa Northe… Inde… 9.63e4 NA NA NA ## 4 CA Canada North … Americ… Northe… Sove… 1.00e7 3.55e7 82.0 43079. ## 5 US United … North … Americ… Northe… Coun… 9.51e6 3.19e8 78.8 51922. ## 6 KZ Kazakhs… Asia Asia Centra… Sove… 2.73e6 1.73e7 71.6 23587. ## 7 UZ Uzbekis… Asia Asia Centra… Sove… 4.61e5 3.08e7 71.0 5371. ## 8 PG Papua N… Oceania Oceania Melane… Sove… 4.65e5 7.76e6 65.2 3709. ## 9 ID Indones… Asia Asia South-… Sove… 1.82e6 2.55e8 68.9 10003. ## 10 AR Argenti… South … Americ… South … Sove… 2.78e6 4.30e7 76.3 18798. ## # … with 167 more rows, 1 more variable: geom &lt;MULTIPOLYGON [°]&gt;, and ## # abbreviated variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion, ## # ⁵​area_km2, ⁶​gdpPercap ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names It is also possible to extract all of the spatial information with several specific functions. For example, st_crs() is used to get information about the coordinate reference system of the given object: st_crs(world) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] We can even extract different CRS definition with: st_crs(world)$wkt ## [1] &quot;GEOGCRS[\\&quot;WGS 84\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n ELLIPSOID[\\&quot;WGS 84\\&quot;,6378137,298.257223563,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n CS[ellipsoidal,2],\\n AXIS[\\&quot;geodetic latitude (Lat)\\&quot;,north,\\n ORDER[1],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n AXIS[\\&quot;geodetic longitude (Lon)\\&quot;,east,\\n ORDER[2],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n USAGE[\\n SCOPE[\\&quot;Horizontal component of 3D system.\\&quot;],\\n AREA[\\&quot;World.\\&quot;],\\n BBOX[-90,-180,90,180]],\\n ID[\\&quot;EPSG\\&quot;,4326]]&quot; st_crs(world)$srid ## [1] &quot;EPSG:4326&quot; st_crs(world)$proj4string ## [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; We can quickly plot the object with the plot() function: plot(world) ## Warning: plotting the first 9 out of 10 attributes; use max.plot = 10 to plot ## all Let’s move to a different data example. What to do when our data is stored in a text file (instead of a spatial file format)? Then, we can read it as a regular data frame with read.csv: cycle_hire_path = system.file(&quot;misc/cycle_hire_xy.csv&quot;, package = &quot;spData&quot;) cycle_hire_txt = read.csv(cycle_hire_path) head(cycle_hire_txt) ## X Y id name area nbikes nempty ## 1 -0.10997053 51.52916 1 River Street Clerkenwell 4 14 ## 2 -0.19757425 51.49961 2 Phillimore Gardens Kensington 2 34 ## 3 -0.08460569 51.52128 3 Christopher Street Liverpool Street 0 32 ## 4 -0.12097369 51.53006 4 St. Chad&#39;s Street King&#39;s Cross 4 19 ## 5 -0.15687600 51.49313 5 Sedding Street Sloane Square 15 12 ## 6 -0.14422888 51.51812 6 Broadcasting House Marylebone 0 18 Next, we need to convert it into a spatial {sf} object with st_as_sf() by providing which columns contain coordinates, and what is the CRS of the data: cycle_hire_xy = st_as_sf(cycle_hire_txt, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = &quot;EPSG:4326&quot;) cycle_hire_xy ## Simple feature collection with 742 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -0.2367699 ymin: 51.45475 xmax: -0.002275 ymax: 51.54214 ## Geodetic CRS: WGS 84 ## First 10 features: ## id name area nbikes nempty ## 1 1 River Street Clerkenwell 4 14 ## 2 2 Phillimore Gardens Kensington 2 34 ## 3 3 Christopher Street Liverpool Street 0 32 ## 4 4 St. Chad&#39;s Street King&#39;s Cross 4 19 ## 5 5 Sedding Street Sloane Square 15 12 ## 6 6 Broadcasting House Marylebone 0 18 ## 7 7 Charlbert Street St. John&#39;s Wood 15 0 ## 8 8 Lodge Road St. John&#39;s Wood 5 13 ## 9 9 New Globe Walk Bankside 3 16 ## 10 10 Park Street Bankside 1 17 ## geometry ## 1 POINT (-0.1099705 51.52916) ## 2 POINT (-0.1975742 51.49961) ## 3 POINT (-0.08460569 51.52128) ## 4 POINT (-0.1209737 51.53006) ## 5 POINT (-0.156876 51.49313) ## 6 POINT (-0.1442289 51.51812) ## 7 POINT (-0.1680743 51.5343) ## 8 POINT (-0.1701345 51.52834) ## 9 POINT (-0.09644075 51.50739) ## 10 POINT (-0.09275416 51.50597) Now, we are able to plot and analyse the data: plot(cycle_hire_xy) We will give more example of the {sf} use in Chapter 4. To learn more read https://journal.r-project.org/archive/2018/RJ-2018-009/RJ-2018-009.pdf and visit https://r-spatial.github.io/sf/. 2.3 Raster data The {terra} package contains classes and methods representing raster objects and operations. It allows raster data to be loaded and saved, provides raster algebra and raster processing, and includes a number of additional functions, e.g., for analysis of terrain characteristics. It also works well on large sets of data. Similarly to {sf}, {terra} also uses many external libraries, but also enables many R packages (Figure ??). To read a spatial raster file, we just need to provide its file path to the rast() function: library(terra) ## terra 1.6.5 raster_filepath = system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) new_raster = rast(raster_filepath) Now, we can look at the summary of our data by typing the object’s name: new_raster ## class : SpatRaster ## dimensions : 457, 465, 1 (nrow, ncol, nlyr) ## resolution : 0.0008333333, 0.0008333333 (x, y) ## extent : -113.2396, -112.8521, 37.13208, 37.51292 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : srtm.tif ## name : srtm ## min value : 1024 ## max value : 2892 It provides us information about the raster dimensions, resolution, CRS, etc. crs() is used to get information about the coordinate reference system of the given {terra} object: crs(new_raster) ## [1] &quot;GEOGCRS[\\&quot;WGS 84\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n ELLIPSOID[\\&quot;WGS 84\\&quot;,6378137,298.257223563,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n CS[ellipsoidal,2],\\n AXIS[\\&quot;geodetic latitude (Lat)\\&quot;,north,\\n ORDER[1],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n AXIS[\\&quot;geodetic longitude (Lon)\\&quot;,east,\\n ORDER[2],\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n ID[\\&quot;EPSG\\&quot;,4326]]&quot; crs(new_raster, describe = TRUE, proj = TRUE) ## name authority code area extent proj ## 1 WGS 84 EPSG 4326 &lt;NA&gt; NA, NA, NA, NA +proj=longlat +datum=WGS84 +no_defs We are also able to create quick visualization with plot(): plot(new_raster) The {terra} package also supports multi-layered raster files. For example, the landsat.tif file has four bands: raster_filepath2 = system.file(&quot;raster/landsat.tif&quot;, package = &quot;spDataLarge&quot;) new_raster2 = rast(raster_filepath2) plot(new_raster2) In this case, we are also able to plot three layers with plotRGB(): plotRGB(new_raster2, r = 3, g = 2, b = 1, stretch = &quot;lin&quot;) Sometimes, we want to create a raster from scratch. This is also possible with the rast() function: my_raster = rast(nrows = 10, ncols = 20, xmin = 0, xmax = 20, ymin = -10, ymax = 0, crs = &quot;EPSG:4326&quot;, vals = 1:200) my_raster ## class : SpatRaster ## dimensions : 10, 20, 1 (nrow, ncol, nlyr) ## resolution : 1, 1 (x, y) ## extent : 0, 20, -10, 0 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : memory ## name : lyr.1 ## min value : 1 ## max value : 200 Let’s plot our new object: plot(my_raster) We will give more example of the {terra} use in Chapter 5. To learn more about the package type `?terra-package``` or visit https://rspatial.github.io/terra/reference/terra-package.html. There is also a second similar function allowing to read spatial vector files called st_read().↩︎ "],["making-maps-in-r.html", "Chapter 3 Making maps in R 3.1 Mapping tools in R 3.2 Basic example 3.3 Shapes and layers 3.4 Attributes layers 3.5 Other map elements 3.6 Interactive mode 3.7 Saving maps 3.8 What else? 3.9 More resources 3.10 Exercises", " Chapter 3 Making maps in R This chapter requires the following packages: library(tmap) library(sf) library(terra) Additionally, we will use three datasets: nz: a set of polygons representing 16 regions of New Zealand nz_ports: a point dataset with locations of New Zealand main ports nz_elev: an elevation raster data of the New Zealand area library(spData) ## ## Attaching package: &#39;spData&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## world data(&quot;nz&quot;) nz_ports &lt;- read_sf(&quot;data/nz_ports.gpkg&quot;) nz_elev &lt;- rast(system.file(&quot;raster/nz_elev.tif&quot;, package = &quot;spDataLarge&quot;)) 3.1 Mapping tools in R R has many packages dedicated to spatial data visualizations. They include tools for making both static maps, interactive maps, specific-purpose maps, animations, etc. Packages for creating static maps include {graphics}, {rasterVis}, {ggplot2}, {ggspatial}, {mapsf}, {**}tidyterra}, etc. Interactive maps’ packages are, for example, {leaflet}, {mapview}, {mapdeck}. Specific-purpose mapping can be achieved with {cartogram} (to construct area cartograms), {geofacet} (“geofaceting”), {geogrid} (to turn polygons into regular or hexagonal grids), and {rayshader} (raytracing to produce 2D and 3D data visualizations). In this workshop, we will focus on the {tmap} package. It accepts spatial data in various formats, allows to create static and interactive maps, and makes it possible to create small multiples map and map animations. 3.2 Basic example The code below shows a basic example of the {tmap} use: tm_shape(nz) + tm_graticules() + tm_polygons(col = &quot;Median_income&quot;, title = &quot;Median income (USD)&quot;) + tm_shape(nz_ports) + tm_symbols(size = 0.75) + tm_scale_bar(breaks = c(0, 100, 200)) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_layout(bg.color = &quot;lightblue&quot;) It combines eight function calls using the + operator to create the above map. Each function can be adjusted using its arguments. We can divide the above tmap functions into a few groups: shapes and layers: tm_shape(), tm_polygons(), tm_symbols(). They are used to read the spatial data and specify how it should be presented. attribute layers: tm_graticules(), tm_scale_bar(), tm_compass(). They add additional information. other map elements: tm_layout(). It specifies the overall map look, including its background color or title. We expand the explanation of these functions in the next few sections. 3.3 Shapes and layers A simple instance of a (t)map consists of specifying spatial object with tm_shape() (this can be, for example, an {sf} vector or a {terra} raster) and then how this object should be visualized. For example, the code below takes the nz object (sf object with polygons) and plots it as polygons: tm_shape(nz) + tm_polygons() Table 3.1 gives a list of basic map layers allowed by {tmap}. We can use different functions depending on our input spatial data type. Table 3.1: Basic map layers Geometry Function polygons tm_polygons() points, lines, and polygons tm_symbols() lines tm_lines() raster tm_raster() points, lines, and polygons tm_text() As we have seen above, polygons can be visualized with tm_polygons(); however, we can also show them using tm_symbols() or tm_text(): tm_shape(nz) + tm_symbols() Each map layer can be adjusted. For example, tm_polygons() can be represented by either: one consistent color (e.g., col = \"darkblue\") unique colors for adjacent polygons (col = \"MAP_COLORS\") color representing values of a given variable (col = Median_income) One consistent color may be set with a color name or a hex color code: tm_shape(nz) + tm_polygons(col = &quot;darkblue&quot;) #or #00008b To use unique colors for adjacent polygons we need to use col = \"MAP_COLORS\": tm_shape(nz) + tm_polygons(col = &quot;MAP_COLORS&quot;) We also can provide a variable name to color each polygon based on its value (note: you can check your variables names with head(nz)). Let’s create a choropleth map of the \"Median_income\" variable: tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;) Next, we customize the map with additional arguments, such as title to change the legend title or palette to provide a name of the color palette to use: tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, title = &quot;Median income (USD)&quot;, palette = &quot;viridis&quot;) The {tmap} package accepts names of a few dozens of color palettes (note: try tmaptools::palette_explorer()), but it is also possible to provide a vector of colors here. By default, {tmap} behaves differently depending on the input variable type, e.g., uses unique colors for categorical variables and pretty breaks for continuous variables. We can use the style argument if we want to change the color breaks. For example, style = \"cont\" creates a continuous color gradient: tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, title = &quot;Median income (USD)&quot;, palette = &quot;viridis&quot;, style = &quot;cont&quot;) Many spatial objects and their related map layers can be connected with the + operator. Importantly, subsequent map layers are drawn on top of the previous ones: tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, title = &quot;Median income (USD)&quot;, palette = &quot;viridis&quot;, style = &quot;cont&quot;) + tm_shape(nz_ports) + tm_symbols() The previous examples used spatial vector data. However, plotting raster data works in the same fashion – we need to provide a spatial object with tm_shape(), and then plot it with tm_raster(): tm_shape(nz_elev) + tm_raster(title = &quot;Elevation (m asl)&quot;, palette = &quot;-Spectral&quot;, style = &quot;cont&quot;) + tm_shape(nz_ports) + tm_symbols() The tmap functions do not only result in a plot, but their output can be also attach to an R object: tm &lt;- tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, title = &quot;Median income (USD)&quot;, palette = &quot;viridis&quot;, style = &quot;cont&quot;) + tm_shape(nz_ports) + tm_symbols() This is useful when we want to add new layers to our map or save it to a file. 3.4 Attributes layers Attributes layers allow to draw often used map elements, such as graticules, scale bars, north arrows, logos, or credits (Table 3.2). Table 3.2: Basic attributes layers Description Function draws latitude and longitude graticules tm_graticules() adds a scale bar tm_scale_bar() adds a compass rose tm_compass() adds a logo tm_logo() adds a text annotation tm_credits() We can add these elements to our previous map with the following code: tm + tm_graticules() + tm_scale_bar(breaks = c(0, 100, 200)) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_logo(&quot;https://foss4g.org/logos/2022-v2.png&quot;) + tm_credits(&quot;N. Roelandt and J. Nowosad&quot;) Each map element can be also customized (e.g., by specifying breaks for the scale bar or a text in the credits), and its location can be set with position. Let’s save our new map to the tm2 object: tm2 &lt;- tm + tm_graticules() + tm_scale_bar(breaks = c(0, 100, 200)) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_logo(&quot;https://foss4g.org/logos/2022-v2.png&quot;) + tm_credits(&quot;N. Roelandt and J. Nowosad&quot;) 3.5 Other map elements The {tmap} also has some other map elements. It includes tm_add_legend() that allows to add a manual legend by specifying its type, color, and title. In the example below, we also use tm_layout() – this function specifies the overall map look, including its background color, title, fonts, etc. tm2 + tm_add_legend(type = &quot;symbol&quot;, col = &quot;grey&quot;, title = &quot;Main ports&quot;) + tm_layout(main.title = &quot;New Zealand&quot;, bg.color = &quot;lightblue&quot;) Here, we save our new map to the tm3 object: tm3 &lt;- tm2 + tm_add_legend(type = &quot;symbol&quot;, col = &quot;grey&quot;, title = &quot;Main ports&quot;) + tm_layout(main.title = &quot;New Zealand&quot;, bg.color = &quot;lightblue&quot;) 3.6 Interactive mode Each map created with {tmap} can be viewed in either \"plot\" and \"view\" mode: the default \"plot\" mode returns a static map, while the \"view\" mode results in an interactive map. We can change the mode with tmap_mode(): tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing Then we just need to open the map object (or write a (t)map code): tm3 ## Credits not supported in view mode. ## Logo not supported in view mode. ## Compass not supported in view mode. ## Warning: In view mode, scale bar breaks are ignored. ## only legends of type &quot;fill&quot; supported in view mode As you can see above, the interactive mode has the same map layers, colors, and legend. However, both modes have their own features. For example, tm_compass(), tm_logo(), and tm_credits() only work in the static mode. On the other hand, the interactive mode allows for zooming or panning, and also makes it possible to select and change the background tiles.2 We can also add some map elements available for interactive mode only, such as tm_minimap() or tm_mouse_coordinates(). tm3 + tm_minimap() + tm_mouse_coordinates() ## Credits not supported in view mode. ## Logo not supported in view mode. ## Compass not supported in view mode. ## Warning: In view mode, scale bar breaks are ignored. ## only legends of type &quot;fill&quot; supported in view mode To return to the static mode, we need to use tmap_mode(\"plot\"): tmap_mode(&quot;plot&quot;) tm3 3.7 Saving maps Maps created with {tmap} can be saved to various file formats. It includes .png for raster graphic files, .svg for vector graphic files, or even .html to save an interactive map. All of the saving can be done with tmap_save(), which accepts the map object and a file path. It also allows customizing the output map resolution. tmap_save(tm3, &quot;my_map.png&quot;) tmap_save(tm3, &quot;my_map.svg&quot;) tmap_save(tm3, &quot;my_map.html&quot;) Note that some {tmap} functions have a tm_ prefix, while other a tmap_ prefix. This syntax allows to distinct between making maps functions (e.g., tm_shape() or tm_polygons()) and other functions (e.g., tmap_save()). 3.8 What else? The above examples showed basic (and probably the most often used) features of {tmap}. However, this package has much more to offer, including: Facet maps: using tm_facets() to create small multiples map Animations: using tm_facets() + tmap_animation() to create map animations Mapping applications: using the {shiny} package with renderTmap(), tmapOutput(), etc. tmap_tip() 3.9 More resources For more resources check the {tmap} repo, read the tmap book (work in progress) and the Making maps with R chapter. 3.10 Exercises Read the following datasets: srtm &lt;- rast(system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;)) zion &lt;- read_sf((system.file(&quot;vector/zion.gpkg&quot;, package = &quot;spDataLarge&quot;))) zion_points &lt;- read_sf(system.file(&quot;vector/zion_points.gpkg&quot;, package = &quot;spDataLarge&quot;)) E1. Create a simple map using {tmap} with three map layers: (1) srtm (colored based on its values), (2) zion (as a grey polygon), (3) zion_points (as grey symbols). E2. Customize the map from E1, for example by: improving the legend title, changing the color palette, etc. E3. Add some attribute layers to your map, including the scale bar and north arrow. Also, add your name in the bottom left corner of the map. E4. Try to adjust the overall map look by, for example, removing the map frame and adding a map title. E5. Bonus: add two manual legend: one for the zion and one for zion_points. E6. Try saving your map to different file formats: .png, .pdf, and .html. Can you notice any difference between the files? Try tm_basemap() and tm_tiles() if you want to customize the interactive tiles.↩︎ "],["manipulating-vector-data.html", "Chapter 4 Manipulating vector data 4.1 Read spatial data 4.2 Reprojection 4.3 Joins 4.4 Aggregation 4.5 Centroids 4.6 Geometric binary predicates 4.7 Saving results", " Chapter 4 Manipulating vector data For this part, we will mostly use {sf} capabilities. Its most spatial functions start with the st_ (for spatial type) prefix like in PostGIS. We will use vector data from the {spData} package. We will also use functions from the {dplyr} package as {sf} objects are data frames compatible with the Tidyverse philosophy. library(sf) library(spData) library(dplyr) library(tmap) library(here) ## here() starts at /Users/runner/work/foss4g2022-getting-started-rspatial/foss4g2022-getting-started-rspatial 4.1 Read spatial data {sf} provides the read_sf() and write_sf() functions to access geospatial files. They can operate with any vector driver provided by GDAL. We will use the data from the {spData} package.3 Let’s see what it contains: list.files(system.file(&quot;shapes&quot;, package = &quot;spData&quot;)) ## [1] &quot;auckland.dbf&quot; &quot;auckland.shp&quot; &quot;auckland.shx&quot; ## [4] &quot;baltim.dbf&quot; &quot;baltim.shp&quot; &quot;baltim.shx&quot; ## [7] &quot;boston_tracts.dbf&quot; &quot;boston_tracts.prj&quot; &quot;boston_tracts.shp&quot; ## [10] &quot;boston_tracts.shx&quot; &quot;columbus.dbf&quot; &quot;columbus.shp&quot; ## [13] &quot;columbus.shx&quot; &quot;cycle_hire_osm.geojson&quot; &quot;cycle_hire.geojson&quot; ## [16] &quot;eire.dbf&quot; &quot;eire.shp&quot; &quot;eire.shx&quot; ## [19] &quot;NY8_bna_utm18.gpkg&quot; &quot;NY8_utm18.dbf&quot; &quot;NY8_utm18.prj&quot; ## [22] &quot;NY8_utm18.shp&quot; &quot;NY8_utm18.shx&quot; &quot;sids.dbf&quot; ## [25] &quot;sids.shp&quot; &quot;sids.shx&quot; &quot;wheat.dbf&quot; ## [28] &quot;wheat.shp&quot; &quot;wheat.shx&quot; &quot;world.dbf&quot; ## [31] &quot;world.gpkg&quot; &quot;world.prj&quot; &quot;world.shp&quot; ## [34] &quot;world.shx&quot; We will work on cycle hires points in London, so let’s start by loading that data. 4.1.1 Cycle hire dataset cycle_hire &lt;- read_sf(system.file(&quot;shapes/cycle_hire.geojson&quot;, package = &quot;spData&quot;)) cycle_hire ## Simple feature collection with 742 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -0.2367699 ymin: 51.45475 xmax: -0.002275 ymax: 51.54214 ## Geodetic CRS: WGS 84 ## # A tibble: 742 × 6 ## id name area nbikes nempty geometry ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;POINT [°]&gt; ## 1 1 River Street Clerkenwell 4 14 (-0.1099705 51.52916) ## 2 2 Phillimore Gardens Kensington 2 34 (-0.1975742 51.49961) ## 3 3 Christopher Street Liverpool S… 0 32 (-0.08460569 51.52128) ## 4 4 St. Chad&#39;s Street King&#39;s Cross 4 19 (-0.1209737 51.53006) ## 5 5 Sedding Street Sloane Squa… 15 12 (-0.156876 51.49313) ## 6 6 Broadcasting House Marylebone 0 18 (-0.1442289 51.51812) ## 7 7 Charlbert Street St. John&#39;s … 15 0 (-0.1680743 51.5343) ## 8 8 Lodge Road St. John&#39;s … 5 13 (-0.1701345 51.52834) ## 9 9 New Globe Walk Bankside 3 16 (-0.09644075 51.50739) ## 10 10 Park Street Bankside 1 17 (-0.09275416 51.50597) ## # … with 732 more rows ## # ℹ Use `print(n = ...)` to see more rows Here we can see a couple of functions : read_sf() is the reading function from {sf} system.file() is a function that allow us to look for data in packages, independently of the operating system By default, {sf} provides informations when loading the dataset. We can see it contains 742 features and 5 fields. It has points with lat/lon coordinates (we can see it through information about its CRS: Geodetic CRS: WGS 84). {sf} also shows the bounding box of our object. Here is the description of the dataset from the documentation: cycle_hire dataset Description: Points representing cycle hire points accross London. Format: id Id of the hire point name Name of the point area Area they are in nbikes The number of bikes currently parked there nempty The number of empty places geometry sfc_POINT Source: cyclehireapp.com/cyclehirelive/cyclehire.csv We can see how many bike are parked, the count of empty slots but not the total amount of bike slots. Let’s create a new slots column for this with mutate() from {dplyr}. cycle_hire &lt;- mutate(cycle_hire, slots = nbikes + nempty) Now, let’s load a polygon data, in this case London’s boroughs stored in the lnd dataset. 4.1.2 Boroughs of London This dataset is stored in an R data format so the loading is different. data(lnd) # load the dataset in memory lnd # call the dataset to visualize the 10 first features ## Simple feature collection with 33 features and 7 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -0.5103751 ymin: 51.28676 xmax: 0.3340155 ymax: 51.69187 ## Geodetic CRS: WGS 84 ## First 10 features: ## NAME GSS_CODE HECTARES NONLD_AREA ONS_INNER SUB_2009 ## 1 Kingston upon Thames E09000021 3726.117 0.000 F &lt;NA&gt; ## 2 Croydon E09000008 8649.441 0.000 F &lt;NA&gt; ## 3 Bromley E09000006 15013.487 0.000 F &lt;NA&gt; ## 4 Hounslow E09000018 5658.541 60.755 F &lt;NA&gt; ## 5 Ealing E09000009 5554.428 0.000 F &lt;NA&gt; ## 6 Havering E09000016 11445.735 210.763 F &lt;NA&gt; ## 7 Hillingdon E09000017 11570.063 0.000 F &lt;NA&gt; ## 8 Harrow E09000015 5046.330 0.000 F &lt;NA&gt; ## 9 Brent E09000005 4323.270 0.000 F &lt;NA&gt; ## 10 Barnet E09000003 8674.837 0.000 F &lt;NA&gt; ## SUB_2006 geometry ## 1 &lt;NA&gt; MULTIPOLYGON (((-0.3306791 ... ## 2 &lt;NA&gt; MULTIPOLYGON (((-0.06402124... ## 3 &lt;NA&gt; MULTIPOLYGON (((0.01213094 ... ## 4 &lt;NA&gt; MULTIPOLYGON (((-0.2445624 ... ## 5 &lt;NA&gt; MULTIPOLYGON (((-0.4118327 ... ## 6 &lt;NA&gt; MULTIPOLYGON (((0.1586928 5... ## 7 &lt;NA&gt; MULTIPOLYGON (((-0.404072 5... ## 8 &lt;NA&gt; MULTIPOLYGON (((-0.404072 5... ## 9 &lt;NA&gt; MULTIPOLYGON (((-0.1965687 ... ## 10 &lt;NA&gt; MULTIPOLYGON (((-0.1998964 ... We can see this dataset has 33 features and 7 fields, also in lat/lon coordinates. The geometry type i, however, different: MULTIPOLYGON. ldn dataset The boroughs of London Description : Polygons representing large administrative zones in London Format: NAME Borough name GSS_CODE Official code HECTARES How many hectares NONLD_AREA Area outside London ONS_INNER Office for national statistics code SUB_2009 Empty column SUB_2006 Empty column geometry sfc_MULTIPOLYGON Source : https://github.com/Robinlovelace/Creating-maps-in-R In order to ease spatial calculations, let’s reproject them. 4.2 Reprojection The Ordnance Survey National Grid is the official one for Great Britain. Its SRID is EPSG:27700. cycle_hire_27700 &lt;- st_transform(cycle_hire, crs = &quot;EPSG:27700&quot;) london_27700 &lt;- st_transform(lnd, crs = &quot;EPSG:27700&quot;) We used st_transform() for the reprojection operation. We can also use st_crs() to check the CRS definition of our objects. Now, we can create a quick map using the the plot() function. This function is part of base R. plot(london_27700$geometry) # we just want to plot the geometry column plot(cycle_hire_27700$geometry, col = &quot;red&quot;, # color cex = 0.5, # size of symbol add = TRUE) # important parameter to create multilayer plots We could also use {tmap} here: tm_shape(london_27700) + tm_borders() + tm_shape(cycle_hire_27700) + tm_symbols(size = 0.5, col = &quot;red&quot;) 4.3 Joins We can use two ways to link those datasets together, by attributes (as they share their area name (area and NAME)) or spatially. For the sake of the exercise, let’s do both. 4.3.1 Join by attributes Let’s join them with a inner join to see how many corresponds. inner_join(cycle_hire_27700, st_drop_geometry(london_27700), # we don&#39;t need the geometry here by = c( &quot;area&quot; = &quot;NAME&quot;) ) ## Simple feature collection with 33 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 523978 ymin: 174408 xmax: 537912.7 ymax: 182972.5 ## Projected CRS: OSGB 1936 / British National Grid ## # A tibble: 33 × 13 ## id name area nbikes nempty geometry slots GSS_CODE ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;POINT [m]&gt; &lt;int&gt; &lt;fct&gt; ## 1 80 Webber Str… Sout… 10 14 (531832.1 179680.3) 24 E09000028 ## 2 108 Abbey Orch… West… 1 27 (529756.5 179341.1) 28 E09000033 ## 3 118 Rochester … West… 12 1 (529528.7 179079.5) 13 E09000033 ## 4 221 Horseferry… West… 12 3 (529880 178976) 15 E09000033 ## 5 240 Colombo St… Sout… 9 5 (531568.5 180203.8) 14 E09000028 ## 6 259 Embankment… West… 0 29 (530351.7 180115.2) 29 E09000033 ## 7 267 Regency St… West… 7 12 (529765.2 178666.4) 19 E09000033 ## 8 281 Smith Squa… West… 4 12 (530077.3 179091.2) 16 E09000033 ## 9 299 Vincent Sq… West… 6 11 (529433.2 178872.2) 17 E09000033 ## 10 302 Putney Pier Wand… 18 10 (523978 175723.3) 28 E09000032 ## # … with 23 more rows, and 5 more variables: HECTARES &lt;dbl&gt;, NONLD_AREA &lt;dbl&gt;, ## # ONS_INNER &lt;fct&gt;, SUB_2009 &lt;fct&gt;, SUB_2006 &lt;fct&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names We can see that only 33 features matched. That’s poor, let’s try this spatially. 4.3.2 Spatial join For this, we will try to provide a GSS_CODE for all cycle hire points. We will regroup the data afterwards. For this, we will select only the GSS_CODE column from london_27700 with the select() function from {dplyr}, the geometry will follow. cycle_hire_27700 &lt;- st_join(cycle_hire_27700, select(london_27700, GSS_CODE)) Now if we look at our dataset, there is a GSS_CODE column. names(cycle_hire_27700) ## [1] &quot;id&quot; &quot;name&quot; &quot;area&quot; &quot;nbikes&quot; &quot;nempty&quot; &quot;geometry&quot; &quot;slots&quot; ## [8] &quot;GSS_CODE&quot; How many points doesn’t have a GSS_code? filter(cycle_hire_27700, is.na(GSS_CODE)) ## Simple feature collection with 1 feature and 7 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 534188 ymin: 180210.4 xmax: 534188 ymax: 180210.4 ## Projected CRS: OSGB 1936 / British National Grid ## # A tibble: 1 × 8 ## id name area nbikes nempty geometry slots GSS_CODE ## * &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;POINT [m]&gt; &lt;int&gt; &lt;fct&gt; ## 1 134 Wapping Hig… Wapp… 20 0 (534188 180210.4) 20 &lt;NA&gt; Only one, that’s more better than before! I don’t know well enough London to fix this. But that is not prevening us from the next steps. Now to paraphrase Anita Graser: “Aggregate all the things!” 4.4 Aggregation 4.4.1 Count # remove NAs cycle_hire_clean &lt;- filter(cycle_hire_27700, !is.na(GSS_CODE)) # let&#39;s put geometry aside cycle_hire_clean &lt;- st_drop_geometry(cycle_hire_clean) # group data by GSS_CODE cycle_hire_grouped &lt;- group_by(cycle_hire_clean, GSS_CODE) # count cycle_hire_by_area &lt;- tally(cycle_hire_grouped, name = &quot;count&quot;, sort= TRUE) # Aggregate cycle_hire_by_area ## # A tibble: 11 × 2 ## GSS_CODE count ## &lt;fct&gt; &lt;int&gt; ## 1 E09000033 171 ## 2 E09000030 117 ## 3 E09000020 90 ## 4 E09000032 59 ## 5 E09000013 58 ## 6 E09000007 57 ## 7 E09000022 46 ## 8 E09000028 40 ## 9 E09000019 37 ## 10 E09000001 35 ## 11 E09000012 31 tally() is equivalent to df %&gt;% summarise(n = n()) 4.4.2 Sum # count cycle_hire_by_area_sum &lt;- summarise( cycle_hire_grouped, # we reused grouped data sum = sum(nbikes), # sums the number of bikes count = n() # count cycle stations ) cycle_hire_by_area_sum ## # A tibble: 11 × 3 ## GSS_CODE sum count ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 E09000001 46 35 ## 2 E09000007 518 57 ## 3 E09000012 606 31 ## 4 E09000013 754 58 ## 5 E09000019 398 37 ## 6 E09000020 872 90 ## 7 E09000022 871 46 ## 8 E09000028 756 40 ## 9 E09000030 1795 117 ## 10 E09000032 1083 59 ## 11 E09000033 1336 171 We could have use the base function aggregate() which works with sf objects. aggregate(cycle_hire_27700[&quot;nbikes&quot;], by = list(cycle_hire_27700$&quot;GSS_CODE&quot;), FUN = sum, na.rm = TRUE) ## Simple feature collection with 11 features and 2 fields ## Attribute-geometry relationship: 0 constant, 1 aggregate, 1 identity ## Geometry type: MULTIPOINT ## Dimension: XY ## Bounding box: xmin: 522502 ymin: 174408 xmax: 538733.2 ymax: 184421 ## Projected CRS: OSGB 1936 / British National Grid ## First 10 features: ## Group.1 nbikes geometry ## 1 E09000001 46 MULTIPOINT ((531116.1 18135... ## 2 E09000007 518 MULTIPOINT ((528392.6 18362... ## 3 E09000012 606 MULTIPOINT ((532312.6 18306... ## 4 E09000013 754 MULTIPOINT ((522502 178727)... ## 5 E09000019 398 MULTIPOINT ((530339.9 18340... ## 6 E09000020 872 MULTIPOINT ((523651 180842)... ## 7 E09000022 871 MULTIPOINT ((528837 176040)... ## 8 E09000028 756 MULTIPOINT ((531235.8 17911... ## 9 E09000030 1795 MULTIPOINT ((533364.4 18174... ## 10 E09000032 1083 MULTIPOINT ((523978 175723.... If we want to represents our data with proportional symbols, we might want to create centroids. {sf} provides two functions in order to do that: st_centroid() st_point_on_surface() st_point_on_surface() assures that every point is in its polygon. That can be useful for irregular shapes where the centroid might be outside the shape. 4.5 Centroids # only keep useful columns boroughs &lt;- select(london_27700, NAME, GSS_CODE) # compute centroids boroughs_centroids &lt;- st_centroid(boroughs) ## Warning in st_centroid.sf(boroughs): st_centroid assumes attributes are constant ## over geometries of x You can also do buffers and other geometrical operations like st_union() to merge geometries. Spatial equivalents of logical operators (???) 4.6 Geometric binary predicates {sf} provides numerous geometric binary predicates that can be used with the intersection function. st_intersects() st_disjoint() st_touches() st_crosses() st_within() st_contains() st_contains_properly() st_overlaps() st_equals() st_covers() st_covered_by() st_equals_exact() st_is_within_distance() You can use it alone or together with st_join(). For example, if we want to get the cycle hires contained in the borough of Wandsworth, we will do it like this: Wandsworth &lt;- filter(london_27700, NAME == &quot;Wandsworth&quot;) (Wandsworth_bike_stations &lt;- st_contains(Wandsworth, cycle_hire_27700) ) ## Sparse geometry binary predicate list of length 1, where the predicate ## was `contains&#39; ## 1: 293, 576, 581, 587, 588, 590, 592, 594, 596, 598, ... That will return a list of cycle hire points id. In contrary, if we want to find in which borough the hire point with id 614 is, we can use the opposite function st_within(): cycle_hire_614 &lt;- filter(cycle_hire_27700, id == &quot;614&quot;) cycle_hire_614_borough &lt;- st_within(cycle_hire_614, london_27700) # borough at index 22 cycle_hire_614_borough ## Sparse geometry binary predicate list of length 1, where the predicate ## was `within&#39; ## 1: 22 To get the borough data, there is some more work to do. london_27700[unlist(cycle_hire_614_borough), ] ## Simple feature collection with 1 feature and 7 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 521054.9 ymin: 170381.5 xmax: 530193.7 ymax: 177893.4 ## Projected CRS: OSGB 1936 / British National Grid ## NAME GSS_CODE HECTARES NONLD_AREA ONS_INNER SUB_2009 SUB_2006 ## 22 Wandsworth E09000032 3522.022 95.6 T &lt;NA&gt; &lt;NA&gt; ## geometry ## 22 MULTIPOLYGON (((523489.6 17... 4.7 Saving results In the first part, we saw that we can read spatial vector data but we can also write it! 4.7.1 Writing data To write data, we will use the st_write() function. It takes the data source name (dsn) as mandatory argument, {sf} will try to find the good driver from the extension (here it is .gpkg for GeoPackage). write_sf() can’t save non geospatial data. So we need to join the data from cycle_hire_by_area_sum to the boroughs first. As we want to save it to GeoPackage4, we also need to provide a layer name: london_boroughs_27700. Repeat for all data you want to save. write_sf( obj = left_join(london_27700, cycle_hire_by_area_sum), # object to write dsn = here(&quot;foss4g_R_workshop.gpkg&quot;), # destination file layer = &quot;london_boroughs_27700&quot;, # layer name append = FALSE) # options ## Joining, by = &quot;GSS_CODE&quot; write_sf( left_join(boroughs_centroids , cycle_hire_by_area_sum), dsn = here(&quot;foss4g_R_workshop.gpkg&quot;), layer = &quot;boroughs_centroids_27700&quot;, append = FALSE) ## Joining, by = &quot;GSS_CODE&quot; write_sf( obj = left_join(cycle_hire_27700, cycle_hire_by_area_sum), dsn = here(&quot;foss4g_R_workshop.gpkg&quot;), layer = &quot;cycle_hire_27700&quot;, append = FALSE) ## Joining, by = &quot;GSS_CODE&quot; We used the here() function as it preserve the project file hierarchy. It works better in RStudio but it is still useful with Jupyter notebooks. The dataset where joined by their GSS_CODE. You can specify the “by” statement, but for the sake of readability, it is not show here. The append = FALSE ensure you can write on existing layer, it is optional. print(here()) # print the project directory list.files(here()) # list the files in the project directory 4.7.2 Check data {sf} provides an st_layers() function that is useful to see the content of a dataset. st_layers(dsn = here(&quot;foss4g_R_workshop.gpkg&quot;)) ## Driver: GPKG ## Available layers: ## layer_name geometry_type features fields ## 1 london_boroughs_27700 Multi Polygon 33 9 ## 2 boroughs_centroids_27700 Point 33 4 ## 3 cycle_hire_27700 Point 742 9 ## crs_name ## 1 OSGB 1936 / British National Grid ## 2 OSGB 1936 / British National Grid ## 3 OSGB 1936 / British National Grid More details on the datasets here: https://cran.r-project.org/web/packages/spData/spData.pdf↩︎ Because GeoPackage are cool !↩︎ "],["manipulating-raster-data.html", "Chapter 5 Manipulating raster data 5.1 Example data 5.2 Map algebra 5.3 Transformations 5.4 Raster-vector interactions 5.5 Raster analysis 5.6 Raster writing 5.7 Exercises", " Chapter 5 Manipulating raster data This chapter requires the following packages: library(tmap) library(sf) library(terra) 5.1 Example data We will read a few datasets for this chapter. It includes srtm.tif – an elevation raster data the Zion National Park area: srtm_path &lt;- system.file(&quot;raster/srtm.tif&quot;, package = &quot;spDataLarge&quot;) srtm &lt;- rast(srtm_path) We can quickly look at this dataset’s summary by just typing its name: srtm ## class : SpatRaster ## dimensions : 457, 465, 1 (nrow, ncol, nlyr) ## resolution : 0.0008333333, 0.0008333333 (x, y) ## extent : -113.2396, -112.8521, 37.13208, 37.51292 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : srtm.tif ## name : srtm ## min value : 1024 ## max value : 2892 We can also extract each information separately using the following functions: ncol(srtm) nrow(srtm) nlyr(srtm) res(srtm) ext(srtm) ## [1] 465 ## [1] 457 ## [1] 1 ## [1] 0.0008333333 0.0008333333 ## SpatExtent : -113.239583212784, -112.85208321281, 37.1320834298579, 37.5129167631658 (xmin, xmax, ymin, ymax) Next, we can quickly look at the data with the plot() function… plot(srtm) … or create more customized map with the {tmap} package: tm_shape(srtm) + tm_graticules() + tm_raster(style = &quot;cont&quot;, title = &quot;elevation (m a.s.l)&quot;, palette = &quot;-Spectral&quot;) + tm_scale_bar(breaks = c(0, 2, 4), text.size = 1) + tm_credits(&quot;N. Roelandt and J. Nowosad&quot;) + tm_layout(inner.margins = 0, main.title = &quot;Zion National Park&quot;) Additional raster dataset we use in this chapter is nlcd.tif – a simplified version of the National Land Cover Database 2011 product for the Zion National Park area. nlcd &lt;- rast(system.file(&quot;raster/nlcd.tif&quot;, package = &quot;spDataLarge&quot;)) 5.2 Map algebra Map algebra is used for a various task related to spatial raster data processing and analysis. It can be divided into four groups of operations: Local - per-cell operations Focal - most often the output cell value is the result of a 3 x 3 input cell block Zonal - to summarize raster values for some zones (usually irregular areas) Global - to summarize raster values for one or several rasters 5.2.1 Local operations Numerical computations are basic examples of local operations – we can create new raster objects by, for example, adding or subtracting existing values: srtm2 &lt;- srtm + 1000 srtm3 &lt;- srtm - 1024 srtm4 &lt;- srtm - 1837 Another example of local operation is when we want to replace some values. This can be done by either subst() or classify(). The subst() function is useful when we want to quickly replace some value(s). srtm_new &lt;- subst(srtm, 1500:2000, NA) On the other hand, the classify() function is more useful when we want to replace many values. It requires at least two arguments – our input raster and a reclassification table. Reclassification table is a matrix usually with two columns (old/new) or three columns (from/to/new): rcl &lt;- matrix(c(0, 1500, 1, 1500, 2000, 2, 2000, 9999, 3), ncol = 3, byrow = TRUE) rcl ## [,1] [,2] [,3] ## [1,] 0 1500 1 ## [2,] 1500 2000 2 ## [3,] 2000 9999 3 srtm_recl &lt;- classify(srtm, rcl = rcl) The last example of local operation is useful when we have many raster layers and want to calculate, for example, spectral indices, such as NDVI. In this case, each cell’s values are treated independently. To showcase local operations on many layers, we will use the landsat.tif dataset. It contains four bands (2, 3, 4, 5) of the Landsat 8 image for the area of Zion National Park. landsat_path &lt;- system.file(&quot;raster/landsat.tif&quot;, package = &quot;spDataLarge&quot;) landsat &lt;- rast(landsat_path) landsat ## class : SpatRaster ## dimensions : 1428, 1128, 4 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 301905, 335745, 4111245, 4154085 (xmin, xmax, ymin, ymax) ## coord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) ## source : landsat.tif ## names : landsat_1, landsat_2, landsat_3, landsat_4 ## min values : 7550, 6404, 5678, 5252 ## max values : 19071, 22051, 25780, 31961 Normalized Difference Vegetation Index (NDVI) is one of the most commonly used spectral index. It uses values of two bands: red (3) and near-infrared (4): \\[ \\begin{split} NDVI&amp;= \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}}\\\\ \\end{split} \\] The above equation can be rewritten as an R function that accepts two arguments, and returns a result of the calculations. ndvi_fun &lt;- function(nir, red){ (nir - red) / (nir + red) } Now, we can apply our ndvi_fun() to the lapp() function. The only important thing we need to remember is to subset our raster to contain only the needed layers: ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun) 5.2.2 Focal operations Focal operations, also known as moving window operations, apply a function for a given window around each cell. A numeric vector or a matrix can represent a window. In the example below, we calculate an average of 3 by 3 cells window for each (focal) cell. srtm_focal_mean &lt;- focal(srtm, w = c(3, 3), fun = &quot;mean&quot;) 5.2.3 Zonal operations Zonal operations are also known as zonal statistics. They calculate summary statistics independently for each provided “zone” (a category in a second raster), and their result is a summary table. In this example, we want to calculate an average elevation (from srtm) for each land cover category (from nlcd). However, firstly we need to have both datasets in the same coordinate reference system. We can do that with the project() function: srtm_utm &lt;- project(srtm, nlcd, method = &quot;bilinear&quot;) srtm_zonal &lt;- zonal(srtm_utm, nlcd, na.rm = TRUE, fun = &quot;mean&quot;) srtm_zonal ## levels srtm ## 1 Water 2227.060 ## 2 Developed 1699.510 ## 3 Barren 1853.950 ## 4 Forest 1996.807 ## 5 Shrubland 1650.796 ## 6 Herbaceous 1644.282 ## 7 Cultivated 1288.272 ## 8 Wetlands 1262.578 5.2.4 Global operations Global operations provide statistics for the entire raster. global(srtm, fun = &quot;mean&quot;) ## mean ## srtm 1842.548 5.3 Transformations Raster transformations can be, in general, divided into two groups: resampling: recalculating raster values for a different grid (e.g., with a different resolution) reprojecting: recalculating raster values for a grid with a different coordinate reference system 5.3.1 Resampling For the resampling example, we need a new grid that we will recalculated values into. Our new grid, new_srtm, will have a lower resolution than the original srtm raster: new_srtm &lt;- srtm res(new_srtm) &lt;- 0.001 new_srtm ## class : SpatRaster ## dimensions : 381, 387, 1 (nrow, ncol, nlyr) ## resolution : 0.001, 0.001 (x, y) ## extent : -113.2396, -112.8526, 37.13208, 37.51308 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) Resampling is applied using the resample() function, which accepts the original raster, new (often empty) grid, and a resampling method’s name (e.g., \"bilinear\"): srtm2 &lt;- resample(srtm, new_srtm, method = &quot;bilinear&quot;) # method! srtm2 ## class : SpatRaster ## dimensions : 381, 387, 1 (nrow, ncol, nlyr) ## resolution : 0.001, 0.001 (x, y) ## extent : -113.2396, -112.8526, 37.13208, 37.51308 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : memory ## name : srtm ## min value : 1032.44 ## max value : 2891.25 To learn more about resampling methods, read the Resampling section of Geocomputation with R. 5.3.2 Reprojecting Reprojecting moves our raster grid into a new coordinate reference system, and then resample its values. CRS of the srtm raster is represented by the code \"EPSG:4326\": crs(srtm, describe = TRUE) ## name authority code area extent ## 1 WGS 84 EPSG 4326 &lt;NA&gt; NA, NA, NA, NA #\\ echo = FALSE # hist(srtm) We can use the project() function to change its CRS. It expects a raster object (that we want to reproject), a new CRS definition5, and a resampling method. In the example below, we are reprojecting the srtm raster into UTM zone 12N: srtm_utm &lt;- project(srtm, &quot;EPSG:32612&quot;, method = &quot;bilinear&quot;) crs(srtm_utm, describe = TRUE) ## name authority code ## 1 WGS 84 / UTM zone 12N EPSG 32612 ## area ## 1 Between 114°W and 108°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Alberta; Northwest Territories (NWT); Nunavut; Saskatchewan. Mexico. United States (USA) ## extent ## 1 -114, -108, 84, 0 5.4 Raster-vector interactions Raster and vector datasets can interact in many ways. It includes: raster cropping and masking by vector polygons extraction of raster values by vector points, lines, and polygons rasterization: converting vector points, lines, polygons to rasters vectorization: converting rasters to polygons or contours Here, we will focus only on raster cropping and masking, and raster extraction by points. Read the raster-vector interactions chapter of the Geocomputation with R book to learn more. 5.4.1 Raster cropping and masking In this example, our goal is to limit the raster data to the area of a polygon. Therefore, we need to read our polygon dataset, zion.gpkg, representing the borders of Zion National Park: zion &lt;- read_sf(system.file(&quot;vector/zion.gpkg&quot;, package = &quot;spDataLarge&quot;)) Raster cropping (crop()) limits the raster data (first argument) extent to the bounding box of the vector data (second argument). srtm_utm_c &lt;- crop(srtm_utm, zion) Raster masking (mask()) is usually done together with cropping. This operation replaces values of all the cells outside the polygon to NA: srtm_utm_m &lt;- mask(srtm_utm_c, zion) 5.4.2 Raster extraction Raster extraction allows to get value(s) of raster cells based on vector data locations. For this example, we will use 30 points located in Zion National Park stored in the zion_points.gpkg file: zion_points &lt;- read_sf(system.file(&quot;vector/zion_points.gpkg&quot;, package = &quot;spDataLarge&quot;)) The extract() function returns a data frame where the ID column represents each vector feature (a point in this case), and the rest of the columns store extracted values. Next, we can connect the extracted values with the vector object using the cbind() function: zion_extract &lt;- terra::extract(srtm, zion_points) zion_points &lt;- cbind(zion_points, zion_extract) zion_points ## Simple feature collection with 30 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -113.2077 ymin: 37.16632 xmax: -112.8717 ymax: 37.43165 ## Geodetic CRS: WGS 84 ## First 10 features: ## ID srtm geom ## 1 1 1802 POINT (-112.9159 37.20013) ## 2 2 2433 POINT (-113.0937 37.39263) ## 3 3 1886 POINT (-113.0246 37.33466) ## 4 4 1370 POINT (-112.9611 37.24326) ## 5 5 1452 POINT (-112.9898 37.20847) ## 6 6 1635 POINT (-112.8807 37.19319) ## 7 7 1380 POINT (-113.0505 37.24061) ## 8 8 2032 POINT (-113.0953 37.34965) ## 9 9 1830 POINT (-113.0362 37.31429) ## 10 10 1860 POINT (-113.2077 37.43165) 5.5 Raster analysis There are many ways to analyze raster data, with some directly implemented in {terra}, while other available in external R packages. Here, we will show to basic examples of – spatial model predictions and spatial segmentation. 5.5.1 Predictions For this example, we will use lsl_sf and ta datasets: - lsl_sf: spatial vector object with several variables, such as lslpts (where TRUE corresponds to an observed landslide ‘initiation point’), slope, cplan (plan curvature), cprof (profile curvature), elev (elevation), log10_carea (catchment area) - ta: spatial raster object with the same variables, except lslpts data(&quot;lsl&quot;, &quot;study_mask&quot;, package = &quot;spDataLarge&quot;) lsl_sf &lt;- st_as_sf(lsl, coords = c(&quot;x&quot;, &quot;y&quot;), crs = &quot;EPSG:32717&quot;) ta &lt;- terra::rast(system.file(&quot;raster/ta.tif&quot;, package = &quot;spDataLarge&quot;)) ta &lt;- mask(ta, study_mask) Our main goal here is to predict landslide susceptibility based on the available data. One possible approach here would be to use Generalized Linear Models (GLM): fit &lt;- glm(lslpts ~ slope + cplan + cprof + elev + log10_carea, family = binomial(), data = lsl_sf) Next, we can use our model, fit, to predict landslide susceptibility for the whole study area with the predict() function: pred &lt;- predict(ta, model = fit, type = &quot;response&quot;) Visit the predict() function help file, ?predict, for more examples of spatial model predictions using {terra}. You can also read about an extended example of spatial prediction in the Statistical learning chapter of Geocomputation with R. 5.5.2 Segmentations Segmentation is a partition of space to identify homogeneous objects. One possible approach to create segments using the SLIC Superpixel algorithm through the {supercells} package. The example data here is an RGB raster with 87,400 cells. library(supercells) ortho &lt;- rast(system.file(&quot;raster/ortho.tif&quot;, package = &quot;supercells&quot;)) Superpixels can be created with the supercells() function that expects our input raster and several parameters: ortho_slic1 &lt;- supercells(ortho, k = 2000, compactness = 10, transform = &quot;to_LAB&quot;) Each superpixel represents a desired level of homogeneity while at the same time maintains boundaries and structures. Superpixels also carry more information than each cell alone, and thus they can speed up the subsequent processing efforts. Next, to vizualize our results we can convert average colors of each superpixel from RGB to a hexadecimal representation: rgb_to_hex &lt;- function(x){ apply(t(x), 2, function(x) rgb(x[1], x[2], x[3], maxColorValue = 255)) } avg_colors &lt;- rgb_to_hex(st_drop_geometry(ortho_slic1[4:6])) # plot(ortho) # plot(st_geometry(ortho_slic1), add = TRUE, col = avg_colors) You can watch the Spatial segmentation in R using the supercells package presentation (slides) to learn more about the {supercells} package. We also encourage you to visit https://cran.r-project.org/view=Spatial and https://cran.r-project.org/package=terra to find other packages allowing for spatial raster data analysis. 5.6 Raster writing Writing raster objects back to the files is possible with writeRaster(). It allows, for example, to provide GDAL driver-specific creation options (gdal), output data type (datatype), or file type (filetype): writeRaster(nlcd, filename = &quot;nlcd0.tif&quot;) writeRaster(nlcd, filename = &quot;nlcd1.tif&quot;, gdal = c(&quot;COMPRESS=NONE&quot;)) writeRaster(nlcd, filename = &quot;nlcd2.tif&quot;, datatype = &quot;INT1U&quot;) writeRaster(nlcd, filename = &quot;nlcd3.tif&quot;, filetype = &quot;COG&quot;) To learn more about raster writing visit the Geocomputation with R book and the official GDAL documentation. 5.7 Exercises Read the following datasets: nz_elev &lt;- rast(system.file(&quot;raster/nz_elev.tif&quot;, package = &quot;spDataLarge&quot;)) data(&quot;nz&quot;, package = &quot;spData&quot;) data(&quot;nz_height&quot;, package = &quot;spData&quot;) E1. Create a map of nz_elev (raster), nz (polygon), and nz_height (point) datasets. E2. Reclassify the nz_elev into three groups of values: below 300 (as 1), between 300 and 600 (as 2), and above 600 m asl (as 3). E3. Extract average elevation for each of the New Zealand region (Hints: read about the fun argument of ?terra::extract; you will also need to add na.rm = TRUE to your function call). Which region has the highest average elevation? Plot the results. E4. Calculate the GNDVI index (see its equation at https://bleutner.github.io/RStoolbox/rstbx-docu/spectralIndices.html) based on the landsat dataset. Compare the obtained values of GNDVI with the previously calculated values of NDVI. E5. Bonus: see the documentation of the shade() function, ?shade, and try to create a hillshade map of New Zealand. This can also be a raster with a different CRS.↩︎ "],["summary.html", "Summary", " Summary Several entry points for users looking for help and information our R-spatial exist. Two informal organizations curate the following websites: r-spatial with a hyphen - https://www.r-spatial.org/: it is a home of {sf} package rspatial without a hyphen - https://www.rspatial.org/: it is a home of {terra} package Additionally, the https://geocompr.github.io/ website is the main home of the Geocomputation with R book. Currently, there are many R-spatial resources available online. It includes many books and extended documentation, for example: The lidR package The tmap book Spatial Data Science with applications in R Geospatial Data Science With R: Applications in Environmental Geography Spatial Modelling for Data Scientists New updates on R and R-spatial may be found at https://www.r-bloggers.com/ and https://rweekly.org/. Twitter can also be a nice place to share and discuss your work with hashtags #rspatial and #geocompr. If you need help, you can ask questions on various websites, including: https://stackoverflow.com/questions/tagged/r https://gis.stackexchange.com/questions/tagged/r https://community.rstudio.com/ https://stat.ethz.ch/mailman/listinfo/r-sig-geo It is just important to remember that conversion between spatial classes is sometimes crucial! Finally, we encourage you to contribute something: Provide suggestions Write bugs reports Make improvements to the documentation, e.g., clarifying unclear sentences, fixing typos Make changes to the code, e.g., to do things in a more efficient way Share your work with #rspatial on Twitter All of the activities improve not only your work, but also the work of many others. Good luck! "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
